{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 크롤링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 기사의 개수:  20\n",
      "101번 코드에 대한 데이터를 만들었습니다.\n",
      "102번 코드에 대한 데이터를 만들었습니다.\n",
      "103번 코드에 대한 데이터를 만들었습니다.\n",
      "105번 코드에 대한 데이터를 만들었습니다.\n",
      "뉴스 기사의 개수:  80\n",
      "/home/aiffel0042/aiffel/news_crawler/news_data.csv File Saved!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "\n",
    "def make_urllist(page_num, code, date): \n",
    "    urllist= []\n",
    "    for i in range(1, page_num + 1):\n",
    "        url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1='+str(code)+'&date='+str(date)+'&page='+str(i)   \n",
    "        news = requests.get(url)\n",
    "\n",
    "        # BeautifulSoup의 인스턴스 생성합니다. 파서는 html.parser를 사용합니다.\n",
    "        soup = BeautifulSoup(news.content, 'html.parser')\n",
    "\n",
    "        # CASE 1\n",
    "        news_list = soup.select('.newsflash_body .type06_headline li dl')\n",
    "        # CASE 2\n",
    "        news_list.extend(soup.select('.newsflash_body .type06 li dl'))\n",
    "        \n",
    "        # 각 뉴스로부터 a 태그인 <a href ='주소'> 에서 '주소'만을 가져옵니다.\n",
    "        for line in news_list:\n",
    "            urllist.append(line.a.get('href'))\n",
    "        return urllist\n",
    "\n",
    "url_list = make_urllist(2, 101, 20200506)\n",
    "print('뉴스 기사의 개수: ',len(url_list))\n",
    "\n",
    "idx2word = {'101' : '경제', '102' : '사회', '103' : '생활/문화', '105' : 'IT/과학'}\n",
    "\n",
    "def make_data(urllist, code):\n",
    "    text_list = []\n",
    "    for url in urllist:\n",
    "        article = Article(url, language='ko')\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        text_list.append(article.text)\n",
    "\n",
    "    #- 데이터프레임의 'news' 키 아래 파싱한 텍스트를 밸류로 붙여줍니다.\n",
    "    df = pd.DataFrame({'news': text_list})\n",
    "\n",
    "    #- 데이터프레임의 'code' 키 아래 한글 카테고리명을 붙여줍니다.\n",
    "    df['code'] = idx2word[str(code)]\n",
    "    return df\n",
    "\n",
    "data = make_data(url_list, 101)\n",
    "#- 상위 10개만 출력해봅니다.\n",
    "data[:10]\n",
    "\n",
    "code_list = [101, 102, 103, 105]\n",
    "\n",
    "code_list\n",
    "\n",
    "def make_total_data(page_num, code_list, date):\n",
    "    df = None\n",
    "\n",
    "    for code in code_list:\n",
    "        url_list = make_urllist(page_num, code, date)\n",
    "        df_temp = make_data(url_list, code)\n",
    "        print(str(code)+'번 코드에 대한 데이터를 만들었습니다.')\n",
    "\n",
    "        if df is not None:\n",
    "            df = pd.concat([df, df_temp])\n",
    "        else:\n",
    "            df = df_temp\n",
    "\n",
    "    return df\n",
    "\n",
    "df = make_total_data(1, code_list, 20200506)\n",
    "\n",
    "print('뉴스 기사의 개수: ',len(df))\n",
    "\n",
    "df.sample(10)\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# 데이터프레임 파일을 csv 파일로 저장합니다.\n",
    "# 저장경로는 이번 프로젝트를 위해 만든 폴더로 지정해 주세요.\n",
    "csv_path = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "if os.path.exists(csv_path):\n",
    "    print('{} File Saved!'.format(csv_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 데이터 전처리 및 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '있', '하', '것', '들', '그', '되', '수', '이', '보', '않', '없', '나', '사람', '주', '아니', '등', '같', '우리', '때', '년', '가', '한', '지', '대하', '오', '말', '일', '그렇', '위하', '때문', '그것', '두', '말하', '알', '그러나', '받', '못하', '일', '그런', '또', '문제', '더', '사회', '많', '그리고', '좋', '크', '따르', '중', '나오', '가지', '씨', '시키', '만들', '지금', '생각하', '그러', '속', '하나', '집', '살', '모르', '적', '월', '데', '자신', '안', '어떤\\t', '내', '경우', '명', '생각', '시간', '그녀', '다시', '이런', '앞', '보이', '번', '나', '다른', '어떻', '여자', '개\\t', '들', '사실', '이렇', '점', '싶', '말', '정도', '좀', '원', '잘', '통하', '소리', '놓\\t']\n",
      "훈련용 뉴스 기사의 개수 : 4595\n",
      "테스트용 뉴스 기사의 개수 :  1532\n",
      "훈련용 레이블의 개수 :  4595\n",
      "테스트용 레이블의 개수 :  1532\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from konlpy.tag import Mecab, Hannanum, Kkma, Komoran, Okt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "csv_path = os.getenv(\"HOME\") + \"/project/aiffel-lms/F3_Crawler/news_data.csv\"\n",
    "csv_path2 = os.getenv(\"HOME\") + \"/project/aiffel-lms/F3_Crawler/news_data2.csv\"\n",
    "stop_path = os.getenv(\"HOME\") + \"/project/aiffel-lms/F3_Crawler/kor_stopwords.txt\"\n",
    "f = open(stop_path, 'r')\n",
    "lines = f.readlines()\n",
    "\n",
    "stopwords = []\n",
    "for line in lines:\n",
    "    line = line.replace('\\n', '')\n",
    "    stopwords.append(line)\n",
    "f.close()\n",
    "\n",
    "print(stopwords)\n",
    "\n",
    "df1 = pd.read_table(csv_path, sep=',')\n",
    "df2 = pd.read_table(csv_path2, sep=',')\n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df['news'] = df['news'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "df['news']\n",
    "\n",
    "df.drop_duplicates(subset=['news'], inplace=True)\n",
    "\n",
    "\n",
    "mecab = Mecab()\n",
    "hannanum = Hannanum()\n",
    "kkma = Kkma()\n",
    "komoran = Komoran()\n",
    "okt = Okt()\n",
    "\n",
    "def preprocessing(data, tokenaizer):\n",
    "    text_data = []\n",
    "\n",
    "    for sentence in data:\n",
    "        temp_data = []\n",
    "        #- 토큰화\n",
    "        temp_data = tokenaizer.morphs(sentence) \n",
    "        #- 불용어 제거\n",
    "        #temp_data = [word for word in temp_data if not word in stopwords] \n",
    "        text_data.append(temp_data)\n",
    "\n",
    "    text_data = list(map(' '.join, text_data))\n",
    "\n",
    "    return text_data\n",
    "\n",
    "mecab_data = preprocessing(df['news'], mecab)\n",
    "#han_data = preprocessing(df['news'], hannanum)\n",
    "#okt_data = preprocessing(df['news'], okt)\n",
    "#kkma_data = preprocessing(df['news'], kkma)\n",
    "#komoran_data = preprocessing(df['news'], komoran)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(mecab_data, df['code'], random_state = 19)\n",
    "\n",
    "print('훈련용 뉴스 기사의 개수 :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수 : ', len(y_train))\n",
    "print('테스트용 레이블의 개수 : ', len(y_test))\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 머신러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.89      0.72      0.79       274\n",
      "          경제       0.94      0.45      0.60       229\n",
      "          사회       0.71      0.94      0.81       552\n",
      "       생활/문화       0.79      0.78      0.79       477\n",
      "\n",
      "    accuracy                           0.78      1532\n",
      "   macro avg       0.83      0.72      0.75      1532\n",
      "weighted avg       0.80      0.78      0.77      1532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "def tfidf_vectorizer(data):\n",
    "    data_counts = count_vect.transform(data)\n",
    "    data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "    return data_tfidf\n",
    "\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 리포트\n",
    "\n",
    "1. 약 2500개의 데이터 샘플로 진행했을 때, IT/과학 분야의 기사 데이터가 부족해 정확도가 제대로 나오지 않음\n",
    "\n",
    "2. 약 3500개의 데이터를 더 추가해 진행하자 정확도가 획기적으로 개선됨\n",
    "\n",
    "3. 불용어 리스트를 임의로 지정한 리스트가 아니라 공개된 공용 불용어 파일을 받아 리스트로 불러옴\n",
    "\n",
    "4. 분석기마다 정확도의 차이는 크지 않음 (0.1미만차이)\n",
    "\n",
    "5. 분석기마다 속도의 차이는 매우 큼(mecab이 가장 빠르고 kkma가 가장 느림)\n",
    "\n",
    "6. 따라서 mecab 분석기를 선택해 진행하였음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
